---
title: "Predicting student debt load"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(message = FALSE)
```

## Load necessary libraries

```{r, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(rcfss)
library(vip)
library(lubridate)

set.seed(123)

theme_set(theme_minimal())
```

## Basic linear model
```{r basic_line}
lm_mod <- linear_reg() %>% 
  set_engine("lm")

debt_rec <- recipe(debt ~ ., data = scorecard) %>% 
  update_role(unitid, name, new_role = "ID") %>%
  step_rm(state, openadmp)

debt_wflow <- workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(debt_rec)

debt_fit <- debt_wflow %>%
  fit(data = scorecard)

predict(object = debt_fit, new_data = scorecard) %>%
  mutate(actual_debt = scorecard$debt) %>%
  rmse(truth = actual_debt, estimate = .pred)
```

Note: for this basic linear model, I could've just altered the dataframe directly to drop my desired variables and used a workflow of model + formula, rather than model + recipe. I chose to do the latter because the recipe approach was more intuitive and neater to me. 

## Cross-validated linear model
```{r line_CV}
# adding folds in 
folds_score <- vfold_cv(scorecard, v = 10)

lm_fit_rs <- debt_wflow %>% 
  fit_resamples(folds_score)

lm_fit_rs %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```

## Tuned decision tree
```{r decision_tree, cache = TRUE}
# gonna try a tuned model
score_split <- initial_split(scorecard)

score_train <- training(score_split)
score_test <- testing (score_split)

tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

tree_folds <- vfold_cv(score_train)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(debt_rec)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = tree_folds,
    grid = tree_grid
    )

tree_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse")

# graph diff values of hyperparameters
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free") +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  scale_x_log10(labels = scales::label_number()) +
  labs(title = "RMSE & R^2 across different hyperparameter values",
       x = "log10(Cost complexity)",
       y = "Mean values of decision tree models",
       color = "Tree depth")
```

It looks like the decision tree model with a tree depth of 4 and a log10(cost complexity) extremely close to 0 was the best. This model has the smallest RMSE (i.e. it has the smallest differences between observed values and model-predicted values of debt) and the highest R^2 (i.e. more of the variance in debt is explained by the model). In short, the model fit is best for those parameter values.


```{r tree_best}
# then choose the best model to finish tuning
best_tree <- tree_res %>%
  select_best("rmse")

best_tree %>%
  select(-.config) %>%
  knitr::kable(
    caption = "Hyperparameter values maximizing accuracy in decision tree model", 
    col.names = c(
      "Complexity parameter",
      "Tree depth"), digits = 10)

final_tree_wf <- tree_wf %>%
  finalize_workflow(best_tree)

# fit best model to training data
final_tree <- final_tree_wf %>%
  fit(data = score_train)

# can see how important each var is here
final_tree %>% 
  pull_workflow_fit() %>% 
  vip() +
  labs(title = "Important variables\nin final decision tree model") 

# last fit & eval of model performance
final_tree_wf %>%
  last_fit(score_split) %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```

The importance graph shows which variables are most important in driving predictions on student loan debt for this model - it definitely makes sense that cost and netcost would be crucial, though I didn't expect completion rate of 1st-time, full-time students to be so important as well.

## Conclusion

Overall, we saw the smallest RMSE for the basic linear model (2992.32). When we add in cross-validation to the linear model, we see a slightly bigger RMSE (3029.57). With the tuned decision tree, we see an even bigger RMSE (3512.67). This seems to make sense. Without resampling or tuning, a model can be unrealistically optimistic and seem to explain data very well - when really the model was overfitting the data we have. 

It is interesting that the tuned decision tree did worse than the cross-validated linear model, so this may be an instance where a decision tree model just doesn't work as well as a linear model. Overall, I'd say that the cross-validated linear model did the best overall, given its comparatively low RMSE.

## Session info

```{r}
devtools::session_info()
```
