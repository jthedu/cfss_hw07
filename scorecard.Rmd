---
title: "Predicting student debt load"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

#do this later
{r setup, echo = FALSE}
#knitr::opts_chunk$set(message = FALSE)


## Load necessary libraries

```{r, echo = FALSE}
library(tidyverse)
library(tidymodels)
library(rcfss)
library(yardstick)
library(skimr)
library(vip)
library(lubridate)

set.seed(123)

theme_set(theme_minimal())
```

ASK WHY fit_resample
also what are "appropriate visualization techniques" for these models
auc, accuracy over tuning, confusion matrix
check distributions of vars w/ ggplot syntax
skimr - package (skimr(df))


```{r basic_line}
lm_mod <- linear_reg() %>% 
  set_engine("lm")

debt_rec <- recipe(debt ~ ., data = scorecard) %>% 
  update_role(unitid, name, new_role = "ID") %>%
  step_rm(state, openadmp)

debt_wflow <- workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(debt_rec)

debt_fit <- debt_wflow %>%
  fit(data = scorecard)

#debt_fit %>%
 # pull_workflow_fit() %>%
  #tidy()

predict(object = debt_fit, new_data = scorecard) %>%
  mutate(actual_debt = scorecard$debt) %>%
  rmse(truth = actual_debt, estimate = .pred)
```
Note: for this basic linear model, I could've just altered the dataframe directly to drop my desired variables and used a workflow of model + formula, rather than model + recipe. I chose to do the latter because the recipe approach seemed more intuitive and neater to me. 

```{r line_CV}
# adding folds in - ISSUE STARTS HERE 
folds_score <- vfold_cv(scorecard, v = 10)

lm_fit_rs <- debt_wflow %>% 
  fit_resamples(folds_score)

lm_fit_rs %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```


```{r decision_tree, cache = TRUE}
#tree_mod <- decision_tree() %>% 
 # set_engine("rpart") %>%
  #set_mode("regression")

# having issues here
#update_model(debt_wflow, tree_mod) %>%
 # fit_resamples(folds) %>%
  #collect_metrics() 

# gonna try a tuned model
score_split <- initial_split(scorecard)

score_train <- training(score_split)
score_test <- testing (score_split)

tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)

#tree_grid %>% 
 # count(tree_depth)

tree_folds <- vfold_cv(score_train)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(debt_rec)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = tree_folds,
    grid = tree_grid
    )

tree_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse")

#check this works
tree_res %>%
  collect_metrics() %>%
 # filter(.metric == "rmse") %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free") +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  scale_x_log10(labels = scales::label_number()) +
 # theme(axis.text.x = element_text(angle = 90)) +
  labs(title = "RMSE & R^2 across different hyperparameter values",
       x = "log10(Cost complexity)",
       y = "Mean values of models",
       color = "Tree depth")
```
looks like model w/ tree depth of 4 & log10(cost complexity) very close to 0 is best -- has the smallest RMSE (i.e. smallest diffs between observed values & predicted values of debt by the model) & highest R^2 (i.e. more of the variance in debt is explained by the model). in short, fit is best for those parameters


```{r tree_best}
# then choose the best model to finish tuning
best_tree <- tree_res %>%
  select_best("rmse")

best_tree %>%
  select(-.config) %>%
  knitr::kable(
    caption = "Hyperparameter values maximizing accuracy in decision tree model", 
    col.names = c(
      "Complexity parameter",
      "Tree depth"), digits = 10)

final_tree_wf <- tree_wf %>%
  finalize_workflow(best_tree)

# fit best model to training data
final_tree <- final_tree_wf %>%
  fit(data = score_train)

# can see how important each var is here
final_tree %>% 
  pull_workflow_fit() %>% 
  vip() +
  labs(title = "Important variables\nin final decision tree model") 

# last fit & eval of model performance
final_tree_wf %>%
  last_fit(score_split) %>%
  collect_metrics() %>%
  filter(.metric == "rmse")
```


so, overall, saw smallest RMSE for basic linear model. w/ cross-validation, saw slightly bigger RMSE & w/ tuned decision tree, saw even bigger. 

makes sense -- w/o resampling & tuning, models can be unrealistically optimistic & seem to explain data very well

## Session info

```{r}
devtools::session_info()
```
