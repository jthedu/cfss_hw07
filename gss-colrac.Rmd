---
title: "Predicting Attitudes Towards Racist College Professors"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(message = FALSE, cache = TRUE)
```

## Load necessary libraries

```{r packages}
library(tidyverse)
library(tidymodels)
library(rcfss)
library(usemodels)
library(kknn)
library(glmnet)
library(xgboost)
library(vip)
library(lubridate)

# load correct gss data frame
data("gss", package = "rcfss")

theme_set(theme_minimal())
```

```{r oof}
# setting seed for all of this doc
set.seed(123)

# splitting data
gss_split <- initial_split(gss, strata = colrac)

gss_train <- training(gss_split)
gss_test <- testing(gss_split)

folds <- vfold_cv(gss_train, v = 10)
```

## Logistic regression
```{r log_reg}
# build a log model specification
lr_mod <- logistic_reg() %>% 
  set_engine("glm")

# resample via a workflow
colrac_lr_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_formula(colrac ~ 
                age + black + degree + partyid_3 + sex + south)

lr_fit_rs <- colrac_lr_wf %>%
  fit_resamples(folds)

lr_fit_rs %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```

## Random forest
```{r random_forest}
# random forest template  
# doesn't actually do anything, but good for reference
#use_ranger(colrac  ~ ., data = gss_train, verbose = TRUE, tune = FALSE)

# build a random forest model specification
rf_mod <- rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_rec <- recipe(colrac ~ ., data = gss_train) %>% 
  update_role(id, wtss, new_role = "ID") %>%
  step_naomit(colrac, skip = TRUE) %>%
  step_medianimpute(all_numeric()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_cut(cohort, breaks = c(1945, 1964, 1980))

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_rs <- rf_wf %>%
  fit_resamples(folds) 

rf_rs %>%
  collect_metrics() %>% 
  filter(.metric == "accuracy")
```

## 5-nearest neighbors
```{r knn}
# template for kknn
#use_kknn(colrac ~ ., data = gss_train, verbose = TRUE, tune = FALSE)

knn_mod <- nearest_neighbor(neighbors = 5) %>%              
  set_engine("kknn") %>%             
  set_mode("classification")      

knn_rec <- recipe(formula = colrac ~ ., data = gss_train) %>% 
  update_role(id, wtss, new_role = "ID") %>%
  step_naomit(colrac, skip = TRUE) %>%
  step_medianimpute(all_numeric()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_cut(cohort, breaks = c(1945, 1964, 1980)) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 

knn_wf <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(knn_rec)

knn_rs <- knn_wf %>%
  fit_resamples(folds) 

knn_rs %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```

## Ridge logistic regression
```{r ridge}
ridge_mod <- logistic_reg(penalty = .01, mixture = 0) %>% 
  set_engine("glmnet") %>% # use glmnet over glm since we have a penalty
  set_mode("classification")

# using same recipe as knn
ridge_rs <- update_model(knn_wf, ridge_mod) %>%
  fit_resamples(folds) 

ridge_rs %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```

## Tuned boosted tree
```{r tune_boost_tree, cache = TRUE}
boost_tune_spec <- 
  boost_tree(
    min_n = tune(),
    tree_depth = tune(), 
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

boost_grid <- grid_regular(min_n(),
                          tree_depth(),
                          learn_rate(),
                          levels = 3)

boost_wf <- workflow() %>%
  add_model(boost_tune_spec) %>%
  add_recipe(knn_rec)

boost_res <- boost_wf %>% 
  tune_grid(
    resamples = folds,
    grid = boost_grid
    )

boost_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")

boost_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(learn_rate, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ min_n, labeller = label_both) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0) +
  scale_x_log10(labels = scales::label_number()) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = "Accuracy across different hyperparameter values",
       x = "log10(Learn rate)",
       y = "Mean accuracy",
       color = "Tree depth")
```

From the graph, we can see that highest accuracy comes from a min_n value of 2. Looking closer, we see that a tree depth of 8 & log10(learn rate) of around .00001 gave the best accuracy of all tested boosted tree models.


```{r boost_best}
# then choose the best model to finish tuning
best_boost <- boost_res %>%
  select_best("accuracy")

best_boost %>%
  select(-.config) %>%
  knitr::kable(
    caption = "Hyperparameter values maximizing accuracy in boosted tree model", 
    col.names = c(
      "Minimum n to split at node",
      "Tree depth", 
      "Learning rate"))

final_boost_wf <- boost_wf %>%
  finalize_workflow(best_boost)

# fit best model to training data
final_boost <- final_boost_wf %>%
  fit(data = gss_train)

# can see how important each var is here
final_boost %>% 
  pull_workflow_fit() %>% 
  vip() +
  labs(title = "Important variables\nin final boosted tree model") 

# last fit & eval of model performance
final_fit <- final_boost_wf %>%
  last_fit(gss_split) 

final_fit %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```

The importance graph shows which variables are most important in driving predictions on the racist college professors question for this model - tolerance seems especially important, which seems logical.


## Comparing models
```{r compare_models}
# writing function
find_accuracy <- function(df, modeltype) {
  df %>%
  collect_metrics %>%
    filter(.metric == "accuracy") %>%
    add_column(type = modeltype) %>%
    select(c(type, mean)) 
}

# assembling table of all accuracy figures
compare_accuracy <- final_fit %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  rename(mean = .estimate) %>%
  add_column(type = "boosted tree") %>%
  select(c(type, mean)) %>%
  bind_rows(
    find_accuracy(lr_fit_rs, "logistic"),
    find_accuracy(rf_rs, "random forest"), 
    find_accuracy(knn_rs, "knn"), 
    find_accuracy(ridge_rs, "ridge logistic")
    ) %>%
  arrange(desc(mean)) 

compare_accuracy %>%
  mutate(mean = mean * 100) %>%
  knitr::kable(
    caption = "Accuracy across models", 
    col.names = c(
      "Model type",
      "Percentage of data predicted correctly"), 
    digits = 2)
```

Random forest gave the most accurate model, predicted about 79% of data correctly. The logistic regression was the worst (53% accuracy), probably because it, unlike all other models built here, didn't use all variables in the dataset as predictor variables. The logistic regression only used the age, black, degree, partyid_3, sex, and south variables as predictors - so less of the variation in the outcome variable would've been explained.

Of the models that did include all available variables as predictors, the 5-nearest neighbors did the worst, with an accuracy of 61%.

Interestingly, tuning didn't help the boosted tree model do better than random forest (tuned boosted tree had a mean accuracy of 76%) or even better than the ridge model (accuracy of 78%), indicating that there's a limit to how far tuning can get you if your model overall doesn't work that well or isn't best suited for the question at hand.


## Session info

```{r}
devtools::session_info()
```
