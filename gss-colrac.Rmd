---
title: "Predicting Attitudes Towards Racist College Professors"
author: "Julia Du"
date: "`r lubridate::today()`"
output: 
  github_document:
    toc: true
---

```{r setup, echo = FALSE}
#knitr::opts_chunk$set(message = FALSE)
```

## Load necessary libraries

```{r packages}
library(tidyverse)
library(tidymodels)
library(rcfss)
library(usemodels)
library(kknn)
library(glmnet)
library(xgboost)

# load correct gss data frame
data("gss", package = "rcfss")
```

```{r initial}
# setting seed for all of this doc
set.seed(123)

# splitting data
gss_split <- initial_split(gss, strata = colrac)

gss_train <- training(gss_split)
gss_test <- testing(gss_split)

folds <- vfold_cv(gss_train, v = 10)
```

CHECK THAT I DON'T NEED TO PRE-PROCESS this logistic model
```{r log_reg}
# build a log model specification
lr_mod <- logistic_reg() %>% 
  set_engine("glm")

# resample via a workflow
colrac_lr_wf <- workflow() %>%
  add_model(lr_mod) %>%
  add_formula(colrac ~ 
                age + black + degree + partyid_3 + sex + south)

lr_fit_rs <- colrac_lr_wf %>%
  fit_resamples(folds)

collect_metrics(lr_fit_rs) %>%
  # can use summarize = FALSE to see metrics for all 10 folds
  filter(.metric == "accuracy")
```
Accuracy pretty dang low, man!

```{r random_forest}
# random forest template  
# doesn't actually do anything, but good for reference
#use_ranger(colrac  ~ ., data = gss_train, verbose = TRUE, tune = FALSE)

# build a random forest model specification
rf_mod <- rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

## do i need TREES IN MY MODEL?

rf_rec <- recipe(colrac ~ ., data = gss_train) %>% 
  update_role(id, wtss, new_role = "ID") %>%
  step_naomit(colrac, skip = TRUE) %>%
  step_medianimpute(all_numeric()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_cut(cohort, breaks = c(1945, 1964, 1980))

rf_wf <- workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

rf_wf %>%
  fit_resamples(folds) %>%
  collect_metrics() %>% 
  filter(.metric == "accuracy")
```


```{r knn}
# template for kknn
#use_kknn(colrac ~ ., data = gss_train, verbose = TRUE, tune = FALSE)


knn_mod <- nearest_neighbor(neighbors = 5) %>%              
  set_engine("kknn") %>%             
  set_mode("classification")      

knn_rec <- recipe(formula = colrac ~ ., data = gss_train) %>% 
  update_role(id, wtss, new_role = "ID") %>%
  step_naomit(colrac, skip = TRUE) %>%
  step_medianimpute(all_numeric()) %>%
  step_modeimpute(all_nominal(), -all_outcomes()) %>%
  step_cut(cohort, breaks = c(1945, 1964, 1980)) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors(), -all_nominal()) 

knn_wf <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(knn_rec)

knn_wf %>%
  fit_resamples(folds) %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```


```{r ridge}
ridge_mod <- logistic_reg(penalty = .01, mixture = 0) %>% 
  set_engine("glmnet") %>% # use glmnet over glm since we have a penalty
  set_mode("classification")

# using same recipe as knn
update_model(knn_wf, ridge_mod) %>%
  fit_resamples(folds) %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```
```{r tune_boost_tree, cache = TRUE}
boost_tune_spec <- 
  boost_tree(
    min_n = tune(),
    tree_depth = tune(), 
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

boost_grid <- grid_regular(min_n(),
                          tree_depth(),
                          learn_rate(),
                          levels = 3)

#tree_grid %>% 
 # count(tree_depth)

boost_wf <- workflow() %>%
  add_model(boost_tune_spec) %>%
  add_recipe(knn_rec)

boost_res <- boost_wf %>% 
  tune_grid(
    resamples = folds,
    grid = boost_grid
    )

boost_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")
```

```{r }
# then choose the best model to finish tuning
best_boost <- boost_res %>%
  select_best("accuracy")

final_boost_wf <- boost_wf %>%
  finalize_workflow(best_boost)

# fit best model to training data
final_boost <- final_boost_wf %>%
  fit(data = gss_train)

final_boost_wf
# best values for
  #min_n = 2
  #tree_depth = 8
  #learn_rate = 3.16227766016838e-06


# can see how important each var is here
library(vip)

final_boost %>% 
  pull_workflow_fit() %>% 
  vip()

# last fit & eval of model performance
final_boost_wf %>%
  last_fit(gss_split) %>%
  collect_metrics() %>%
  filter(.metric == "accuracy")

```

## Session info

```{r}
devtools::session_info()
```
